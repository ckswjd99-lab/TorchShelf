{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PoC: Logarithmic Query Number is Sufficient for ZOO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from import_shelf import shelf\n",
    "from shelf.models.transformer import VisionTransformer\n",
    "from shelf.dataloaders.cifar import get_CIFAR10_dataset\n",
    "from shelf.trainers.zeroth_order import learning_rate_estimate_second_order\n",
    "from shelf.trainers.classic import validate\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HYPERPARAMS ###\n",
    "\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 512\n",
    "IMAGE_SIZE = 32\n",
    "PATCH_SIZE = 4\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "LR_MAX = 1e-2\n",
    "LR_MIN = 1e-5\n",
    "SMOOTHING = 5e-4\n",
    "QUERY_BASE = 1.05\n",
    "NUM_QUERY = 1\n",
    "MOMENTUM = 0.8\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    \"dim\": 128,\n",
    "    \"depth\": 4,\n",
    "    \"heads\": 2,\n",
    "    \"mlp_dim\": 128,\n",
    "    \"dropout\": 0.1,\n",
    "    \"emb_dropout\": 0.1,\n",
    "}\n",
    "# MODEL_CONFIG = {\n",
    "#     \"dim\": 512,\n",
    "#     \"depth\": 4,\n",
    "#     \"heads\": 6,\n",
    "#     \"mlp_dim\": 256,\n",
    "#     \"dropout\": 0.1,\n",
    "#     \"emb_dropout\": 0.1,\n",
    "# }\n",
    "\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "PATH_MODEL = './saves/zoo_poc.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA LOADING ###\n",
    "\n",
    "train_loader, val_loader = get_CIFAR10_dataset(batch_size=BATCH_SIZE)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "plt.figure(figsize=(10, 1))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(train_loader.dataset.data[i])\n",
    "    plt.title(classes[train_loader.dataset.targets[i]])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL ###\n",
    "\n",
    "model = VisionTransformer(\n",
    "    image_size=IMAGE_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    **MODEL_CONFIG\n",
    ").to(DEVICE)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "num_groups = int(np.log(num_params)/np.log(QUERY_BASE))\n",
    "print(f\"Number of parameters: {num_params}\")\n",
    "print(f\"Number of groups: {num_groups}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OTHERS ###\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS, eta_min=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def gradient_estimate_groupwise(input, label, model, criterion, num_groups, group_dict, query=1, smoothing=5e-4):\n",
    "    model.eval()\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    # Prepare the result dictionary\n",
    "    result_gradient = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad: continue\n",
    "        result_gradient[name] = torch.zeros_like(param.data)\n",
    "\n",
    "    # Original loss\n",
    "    loss_original = criterion(model(input), label)\n",
    "\n",
    "    # Perturb and measure the loss\n",
    "    for g in range(num_groups):\n",
    "        for q in range(query):\n",
    "            estimated_gradient = {}\n",
    "\n",
    "            # perturb the model\n",
    "            for name, param in model.named_parameters():\n",
    "                if not param.requires_grad: continue\n",
    "                estimated_gradient[name] = torch.normal(mean=0, std=1, size=param.data.size(), device=param.data.device, dtype=param.data.dtype)\n",
    "                \n",
    "                # handle pruned parameters\n",
    "                if '_orig' in name and name.replace('_orig', '_mask') in state_dict:\n",
    "                    mask = state_dict[name.replace('_orig', '_mask')]\n",
    "                    estimated_gradient[name] *= mask\n",
    "                \n",
    "                # handle grouping\n",
    "                estimated_gradient[group_dict[name] != g] = 0\n",
    "                \n",
    "                # add the perturbation\n",
    "                param.data += estimated_gradient[name] * smoothing\n",
    "            \n",
    "            # measure the loss\n",
    "            loss_perturbed = criterion(model(input), label)\n",
    "\n",
    "            # restore the model\n",
    "            for name, param in model.named_parameters():\n",
    "                if not param.requires_grad: continue\n",
    "                param.data -= estimated_gradient[name] * smoothing\n",
    "            \n",
    "            # accumulate the gradient\n",
    "            loss_difference = (loss_perturbed - loss_original) / smoothing\n",
    "            for name, param in model.named_parameters():\n",
    "                if not param.requires_grad: continue\n",
    "                result_gradient[name] += loss_difference * estimated_gradient[name]\n",
    "    \n",
    "    # Average the gradient\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad: continue\n",
    "        result_gradient[name] /= query\n",
    "    \n",
    "    return result_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_r(N, d):\n",
    "    equation = np.poly1d([1] + [0 for _ in range(N-1)] + [-d, d-1], False)\n",
    "    roots = np.roots(equation)\n",
    "    roots = roots[np.isreal(roots)]\n",
    "    r = np.real(np.max(roots))\n",
    "\n",
    "    if r <= 1:\n",
    "        raise ValueError(\"r must be greater than 1\")\n",
    "\n",
    "    return r\n",
    "\n",
    "def group_by_gradient_exp(estimated_gradient, num_groups):\n",
    "    all_gradients = torch.cat([grad.flatten() for grad in estimated_gradient.values()]).abs()\n",
    "    num_params = all_gradients.size(0)\n",
    "\n",
    "    # Calculate r\n",
    "    r = calc_r(num_groups, num_params)\n",
    "\n",
    "    # Fine milestones\n",
    "    milestones = []\n",
    "    for group_idx in range(num_groups):\n",
    "        group_ratio = (r ** (group_idx + 1) - 1) / (r - 1) / num_params\n",
    "        milestones.append(torch.quantile(all_gradients, group_ratio, interpolation='lower'))\n",
    "    milestones[-1] = torch.quantile(all_gradients, 1.0)\n",
    "\n",
    "    # Group the parameters\n",
    "    group_dict = {}\n",
    "    for name, grad in estimated_gradient.items():\n",
    "        group_dict[name] = torch.zeros_like(grad)\n",
    "        for group_idx, milestone in enumerate(milestones):\n",
    "            group_dict[name][grad.abs() > milestone] = group_idx\n",
    "\n",
    "    return group_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_zo(\n",
    "        train_loader, model, criterion, optimizer, epoch,\n",
    "        smoothing=1e-3, query=1, lr_auto=True, lr_max=1e-2, lr_min=1e-5, momentum=0.9,\n",
    "        num_groups=1, group_dict=None,\n",
    "        config=None\n",
    "    ):\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare statistics\n",
    "    num_data = 0\n",
    "    num_correct = 0\n",
    "    sum_loss = 0\n",
    "    num_query = 0\n",
    "    \n",
    "    lr_history = []\n",
    "\n",
    "    # Prepare grouping\n",
    "    if num_groups > 1 and group_dict is None:\n",
    "        group_dict = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad: continue\n",
    "            group_dict[name] = torch.randint(0, num_groups, size=param.data.size(), device=param.data.device, dtype=torch.long)\n",
    "    \n",
    "    gradient_momentum = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad: continue\n",
    "        gradient_momentum[name] = torch.zeros_like(param.data)\n",
    "\n",
    "    # Train the model\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}', leave=False)\n",
    "    for input, label in pbar:\n",
    "        input = input.cuda()\n",
    "        label = label.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Gradient estimation\n",
    "        estimated_gradient = gradient_estimate_groupwise(input, label, model, criterion, num_groups, group_dict, query, smoothing)\n",
    "        num_query += query * num_groups\n",
    "\n",
    "        # Update momentum\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad: continue\n",
    "            gradient_momentum[name] = momentum * gradient_momentum[name] + estimated_gradient[name]\n",
    "\n",
    "        # Apply gradient\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad: continue\n",
    "            param.grad = estimated_gradient[name]\n",
    "\n",
    "        # Estimate learning rate\n",
    "        lr = lr_min\n",
    "        if lr_auto:\n",
    "            lr = learning_rate_estimate_second_order(input, label, model, criterion, estimated_gradient, smoothing=smoothing)\n",
    "            lr = abs(lr.item()) if lr != 0 else lr_min\n",
    "\n",
    "            num_query += 3\n",
    "\n",
    "        lr = min(lr, lr_max)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        lr_history.append(lr)\n",
    "\n",
    "        # Update group\n",
    "        group_dict = group_by_gradient_exp(gradient_momentum, num_groups)\n",
    "        \n",
    "        # Update the model\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        output = model(input)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        num_data += label.size(0)\n",
    "        num_correct += (predicted == label).sum().item()\n",
    "        sum_loss += loss.item() * label.size(0)\n",
    "    \n",
    "        accuracy = num_correct / num_data\n",
    "        avg_loss = sum_loss / num_data\n",
    "\n",
    "        pbar.set_postfix(train_accuracy=accuracy, train_loss=avg_loss)\n",
    "        \n",
    "    accuracy = num_correct / num_data\n",
    "    avg_loss = sum_loss / num_data\n",
    "\n",
    "    if config is not None:\n",
    "        config['lr_avg'] = np.mean(lr_history)\n",
    "        config['lr_std'] = np.std(lr_history)\n",
    "        config['num_query'] = num_query\n",
    "        config['group_dict'] = group_dict\n",
    "\n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_dict = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    config = {}\n",
    "\n",
    "    num_groups = int(np.log(num_params)/np.log(QUERY_BASE))\n",
    "    \n",
    "    train_acc, train_loss = train_zo(\n",
    "        train_loader, model, criterion, optimizer, epoch,\n",
    "        smoothing=SMOOTHING, query=NUM_QUERY, lr_auto=True, lr_max=LR_MAX, lr_min=LR_MIN, momentum=MOMENTUM,\n",
    "        num_groups=num_groups, group_dict=group_dict,\n",
    "        config=config\n",
    "    )\n",
    "    val_acc, val_loss = validate(val_loader, model, criterion, epoch)\n",
    "\n",
    "    lr_avg = config['lr_avg']\n",
    "    lr_std = config['lr_std']\n",
    "    group_dict = config['group_dict']\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:3d}/{EPOCHS}, \"\n",
    "        f\"LR: {lr_avg:.6f}±{lr_std:.6f} | \"\n",
    "        f\"Train Acc: {train_acc * 100:.2f}%, \"\n",
    "        f\"Train Loss: {train_loss:.4f}, \"\n",
    "        f\"Val Acc: {val_acc*100:.2f}%, \"\n",
    "        f\"Val Loss: {val_loss:.4f} | \"\n",
    "        f\"Num Query: {config['num_query']}, \"\n",
    "        f\"Time: {time.time() - start_time:.3f}s\"\n",
    "    )\n",
    "\n",
    "torch.save(model.state_dict(), PATH_MODEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
